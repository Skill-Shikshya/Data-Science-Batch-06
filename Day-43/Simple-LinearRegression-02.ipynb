{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f57786-dc27-4eb9-ad54-6aa100d7cab5",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares with Simple Linear Regression\n",
    "\n",
    "\n",
    "The simple linear regression model is:\n",
    "\n",
    " $$\\mathbf{y} = \\beta_0 +\\beta_1\\mathbf{x}$$\n",
    "\n",
    "\n",
    "where, we need to estimate the parameters, intercept($\\beta_0$) and slope($\\beta_1$). \n",
    "\n",
    "\n",
    "Let's recall an Advertising dataset and simple linear regression performed on scatter plot of _sales_ Vs. _TV_. With the help of `Scikit-Learn`, we were able to fit the best regression line among all the possibilities. Here is a snapshot:\n",
    "\n",
    "<figure align=\"center\">\n",
    "       <img src=\"fig1.png\" height=\"350\" width=\"600\">\n",
    "       <figcaption>Figure 1: Simple Linear Regression </figcaption>\n",
    "   </figure>\n",
    "\n",
    "\n",
    "\n",
    "The blue line is a simple linear regression line with output $\\mathbf{y}$ as `sales` and $\\mathbf{x}$ as `TV`. The residual or error, $\\epsilon$ is the difference between the observed value, $y_i$, and predicted value, $\\hat{y_i}$. The observed value is the actual output data point, which is all blue dots in the figure, and the predicted value is the point given by the black regression line. Error for each output data point is shown by the vertical distance from the actual output data point to the predicted point on a regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5543bc9-106d-4645-bb36-1cd473019dc9",
   "metadata": {},
   "source": [
    "The predicted output value is:\n",
    "\n",
    "$$\\hat{y_i} = \\beta_0 + \\beta_1x_i$$\n",
    "\n",
    "The observed (actual) output value is:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i$$\n",
    "\n",
    "Where $\\epsilon_i$ is a random error, not a parameter. The error $\\epsilon_i$ as ($y_{i}-\\hat{y_{i}}$) can either be positive or negative or even 0 sometimes. As we can see in the figure, vertical lines are on either side of the regression line. To avoid the cancellation of the error while summing errors, we square each error and sum them, called _Residual Sum of Squares (RSS)_ or _Sum of Squared Errors (SSE)_.\n",
    "\n",
    "$$\\text{Sum of Squared Errors (SSE)} = \\sum_{i=1}^{n}(y_{i}-\\hat{y_{i}})^2$$\n",
    "\n",
    "The summation is indexed from $1$ to $n$, since we have $n$ samples. Sum of Squared Errors (SSE) is the function of $\\beta_0$ and $\\beta_1$. We can also take it as _Loss function_. The main principle of Least Squares is that we should end up choosing intercept ($\\beta_0$) and slope ($\\beta_1$) such that the overall sum is minimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1dfe87-9406-4d82-bab1-137b9b398097",
   "metadata": {},
   "source": [
    "\n",
    "Thus, to estimate the parameters, we minimize the sum of squared error. Sum of Squared Errors (SSE) can also be written as:\n",
    "\n",
    "$$\\text{SSE} = \\sum_{i=1}^{n}(y_{i}-\\hat{y_{i}})^2 =\\sum_{i=1}^{n}(y_{i}-(\\beta_0+\\beta_1x_i))^2 $$\n",
    "\n",
    "\n",
    "\n",
    "$\\hat{y_i}$ is replaced with the simple linear regression model equation. Since we tend to minimize $\\text{SSE}$, it is also called an objective function. Since the objective function, $\\text{SSE}$ is a squared term, it is always positive. If we plot objective function, it would be a convex graph facing upwards. \n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "       <img src=\"./fig2.png\" height=\"400\" width=\"500\">\n",
    "       <figcaption>Figure 2: Convex cost function </figcaption>\n",
    "   </figure>\n",
    "\n",
    "\n",
    "The parameters at a minimum point are obtained from calculus by setting the first derivative of the objective function to $0$. Gradient or slope is always $0$ at the minimum point. We have two unknown parameters, intercept ($\\beta_0$) and slope ($\\beta_1$) so, we will take the partial derivative of _SSE_ with respect to $\\beta_0$ and $\\beta_1$ separately. We will set both partial derivatives to 0 and solve for $\\beta_0$ and $\\beta_1$ separately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35c8587-5338-4a70-9480-c3f8484d95b2",
   "metadata": {},
   "source": [
    "Taking partial derivatives with respect to $\\beta_0$:\n",
    "\n",
    "$$\\frac{\\partial\\ \\text{SSE}}{\\partial \\beta_0}  = \\frac{\\partial }{\\partial \\beta_0}\\sum(y_i-(\\beta_0+\\beta_1x_i))^2$$\n",
    "\n",
    "Note that the derivative of the sum is the sum of the derivatives. So, we can take the derivative inside the summation.\n",
    "\n",
    "$$\\frac{\\partial }{\\partial \\beta_0}\\sum(y_i-(\\beta_0+\\beta_1x_i))^2 = \\sum\\frac{\\partial }{\\partial \\beta_0}(y_i-(\\beta_0+\\beta_1x_i))^2 $$\n",
    "\n",
    "Now, applying power rule and chain rule, we get:\n",
    "\n",
    "\n",
    "$$= \\sum2(y_i-(\\beta_0+\\beta_1x_i))(-1) $$\n",
    "\n",
    "$$=-2\\sum(y_i-(\\beta_0+\\beta_1x_i)) ......(1)$$\n",
    "\n",
    "\n",
    "\n",
    "Now, with respect to $\\beta_1$:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial\\ {\\text{SSE}} }{\\partial \\beta_1} = \\frac{\\partial }{\\partial \\beta_1}\\sum(y_i-(\\beta_0+\\beta_1x_i))^2$$\n",
    "\n",
    "Again, the derivative of the sum is the sum of the derivatives, So, we take the derivative inside the summation.\n",
    "\n",
    "$$\\frac{\\partial }{\\partial \\beta_1}\\sum(y_i-(\\beta_0+\\beta_1x_i))^2 = \\sum\\frac{\\partial }{\\partial \\beta_1}(y_i-(\\beta_0+\\beta_1x_i))^2 $$\n",
    "\n",
    "\n",
    "Applying power rule, $2$ comes out front and exponent becomes $1$. We will also apply chain rule to encounter the coefficient of $\\beta_1$. \n",
    "$$= \\sum2(y_i-(\\beta_0+\\beta_1x_i))(-x_i) $$\n",
    "\n",
    "Cleaning up a bit, \n",
    "\n",
    "$$= -2\\sum x_i(y_i-(\\beta_0+\\beta_1x_i)) ......(2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e797a48e-bd95-4f28-bf3e-c57385dffb1e",
   "metadata": {},
   "source": [
    "Now, we set up the partial derivatives equal to $0$ for equation $(1)$ and $(2)$.\n",
    "\n",
    "\n",
    "$$-2\\sum(y_i-(\\beta_0+\\beta_1x_i))  = 0$$\n",
    "\n",
    "$$-2\\sum x_i(y_i-(\\beta_0+\\beta_1x_i))  = 0$$\n",
    "\n",
    "Here, we have two equations and two unknowns, and we are going to solve this to find our parameters. But how do we get that?\n",
    "\n",
    "First, we will get an expression for $\\beta_0$ from the first equation. That expression would involve $\\beta_1$, and we will substitute that equation in the second equation and solve for $\\beta_1$. Let's solve the first equation.\n",
    "\n",
    "\n",
    "Solving for $\\beta_0$ equating equation $\\text{(1)}$ to $0$,\n",
    "\n",
    "\n",
    "$$-2\\sum(y_i-(\\beta_0+\\beta_1x_i))  = 0$$\n",
    "\n",
    "We can divide both sides by $-2$ so that we get,\n",
    "\n",
    "$$\\sum(y_i-(\\beta_0+\\beta_1x_i))  = 0$$\n",
    "\n",
    "If we carry the summation term through each terms inside the bracket, we get:\n",
    "\n",
    "$$\\sum y_i- \\sum \\beta_0 - \\sum \\beta_1x_i = 0$$\n",
    "\n",
    "Note that with respect to summation, $\\beta_0 $ and $\\beta_1$ are constants. Statistically, they are random variables that take on any random value. But the values they take are constant over the samples. With respect to summation over the samples, they are constants so they can come outside the summation term as:\n",
    "\n",
    "$$\\sum y_i- n\\beta_0 - \\beta_1\\sum x_i = 0$$\n",
    "\n",
    "The sum of $\\beta_0$ from $1$ to $n$ turns to $n\\beta_0$ and $\\beta_1$ comes out of the summation term.\n",
    "\n",
    "Now, isolating the $n\\beta_0$ term, we get:\n",
    "\n",
    "$$n\\beta_0 = \\sum y_i- \\beta_1\\sum x_i$$\n",
    "\n",
    "Dividing both sides by $n$, we get:\n",
    "\n",
    "$$\\beta_0 = \\frac{\\sum y_i}{n}- \\frac{\\beta_1\\sum x_i}{n}$$\n",
    "\n",
    "The sum of all $y's$ divided by $n$ gives the mean or average and so is for $x's$.So, we end up with:\n",
    "\n",
    "\n",
    "$$\\beta_0 = \\overline{y}- \\beta_1\\overline{x}$$\n",
    "\n",
    "\n",
    "But this doesn't work without knowing the value of $\\beta_1$. So, we substitute this expression of $\\beta_0$ to the equation where the partial derivative of $\\beta_1$ is set to $0$.\n",
    "\n",
    "\n",
    "Hence, solving for $\\beta_1$,\n",
    "\n",
    "$$-2\\sum x_i(y_i-(\\beta_0+\\beta_1x_i))  = 0$$\n",
    "\n",
    "\n",
    "We can divide both sides by $-2$ so that we get,\n",
    "\n",
    "$$\\sum x_i(y_i-(\\beta_0+\\beta_1x_i))  = 0$$\n",
    "\n",
    "Substituting $\\beta_0$ with $\\overline{y}- \\beta_1\\overline{x}$, we get:\n",
    "\n",
    "$$\\sum x_i(y_i-(\\overline{y}- \\beta_1\\overline{x}+\\beta_1x_i))  = 0$$\n",
    "\n",
    "Now, we are getting somewhere since the unknown in the above expression is only $\\beta_1$. Now, we will find a way to isolate $\\beta_1$. Let's first gather similar terms together, i.e., putting $y_i$ with $\\overline{y}$ and $x_i$ with $\\overline{x}$:\n",
    "$$\\sum x_i((y_i-\\overline{y})- \\beta_1(x_i- \\overline{x}))  = 0$$\n",
    "\n",
    "\n",
    "Carrying summation through each terms, we get:\n",
    "$$\\sum x_i(y_i-\\overline{y})- \\sum\\beta_1x_i(x_i- \\overline{x})\n",
    "  = 0$$\n",
    "\n",
    "$\\beta_1$ is a constant, so we put it outside the summation term.\n",
    "$$\\sum x_i(y_i-\\overline{y})- \\beta_1\\sum x_i(x_i- \\overline{x})= 0$$\n",
    "\n",
    "Moving the term including $\\beta_1$ to the other side, we get;\n",
    "$$\\sum x_i(y_i-\\overline{y})= \\beta_1\\sum x_i(x_i- \\overline{x})$$\n",
    "\n",
    "Now, we express $\\beta_1$ as:\n",
    "\n",
    "$$\\beta_1 = \\frac{\\sum x_i(y_i-\\overline{y})}{\\sum x_i(x_i- \\overline{x})} ......(3)$$\n",
    "\n",
    "\n",
    "This is one way of expressing $\\beta_1$, but we usually don't follow this fashion. We can also write the expression of $\\beta_1$ as:\n",
    "\n",
    "$$\\beta_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} ...... (4)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065970ef-f729-46af-825a-01019029f7b3",
   "metadata": {},
   "source": [
    "\n",
    "From equation $(3)$ and $(4)$, we can see that:\n",
    "\n",
    " $$\\sum x_i(y_i-\\overline{y}) = \\sum(x_i - \\bar{x})(y_i - \\bar{y})$$\n",
    "\n",
    " $$\\sum x_i(x_i-\\overline{x}) = \\sum (x_i - \\bar{x})^2$$\n",
    "\n",
    "Now, we will see how these two expressions are equivalent.\n",
    "\n",
    "__For the numerator part:__\n",
    "\n",
    "$$\\sum(x_i - \\overline{x}) (y_i - \\overline{y})=  \\sum x_i(y_i-\\overline{y}) - \\sum \\overline{x}(y_i-\\overline{y})$$\n",
    "\n",
    "$\\overline{x}$ is a constant term so we take it out:\n",
    "\n",
    "$$\\sum(x_i - \\overline{x}) (y_i - \\overline{y})=  \\sum x_i(y_i-\\overline{y}) - \\overline{x}\\sum (y_i-\\overline{y})$$\n",
    "\n",
    "Now, let's see the second term of the equation $\\sum (y_i-\\overline{y})$.\n",
    "\n",
    "$$\\sum (y_i-\\overline{y}) = \\sum y_i - \\sum\\overline{y} = \\sum y_i - n\\overline{y} = 0$$\n",
    "\n",
    "Since, $n\\overline{y}$ is equal to  $\\sum y_i$, the whole second term becomes $0$. Hence\n",
    "$$\\sum(x_i - \\overline{x}) (y_i - \\overline{y})=  \\sum x_i(y_i-\\overline{y})$$\n",
    "\n",
    "__For the denominator part:__\n",
    "\n",
    "$$\\sum (x_i - \\overline{x})^2 = \\sum (x_i-\\overline{x})(x_i-\\overline{x})$$\n",
    "\n",
    "\n",
    "$$\\sum (x_i - \\overline{x})^2 = \\sum x_i(x_i-\\overline{x})- \\sum \\overline{x}(x_i-\\overline{x})$$\n",
    "\n",
    "Again, $\\overline{x}$ is a constant term, so we take it out:\n",
    "\n",
    "$$\\sum (x_i - \\overline{x})^2 = \\sum x_i(x_i-\\overline{x})- \\overline{x}\\sum (x_i-\\overline{x})$$\n",
    "\n",
    "Again, as earlier, let's see the second term $\\sum (x_i-\\overline{x})$.\n",
    "\n",
    "$$\\sum (x_i-\\overline{x}) = \\sum x_i - \\sum\\overline{x} = \\sum x_i - n\\overline{x} = 0 $$\n",
    "\n",
    "\n",
    "\n",
    "Since, $\\overline{x}=\\frac{\\sum x_i}{n}\\ $, $\\sum x_i = n\\overline{x}$ so the second term becomes $0$. Hence\n",
    "\n",
    "$$\\sum (x_i - \\overline{x})^2 = \\sum x_i(x_i-\\overline{x})$$\n",
    "\n",
    "So, now we proved the similarity of the denominator and numerator terms of both expressions of $\\beta_1$. \n",
    "\n",
    "Since the parameters are estimates, we usually put _hats_ on them. The key equations of the estimated parameters for simple linear regression are:\n",
    "\n",
    "$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$\n",
    "\n",
    "$$\\hat{\\beta_0} = \\bar{y} - \\beta_1\\bar{x}$$\n",
    "From the samples provided, first we find $\\beta_1$ from the first expression and substitute the value of $\\beta_1$ in the second expression for $\\beta_0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f298d5-b446-47c1-bc19-08e50f95ff4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
